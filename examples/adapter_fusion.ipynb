{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYZH02h-JeX5"
      },
      "source": [
        "# **AdapterFusion for Sequence Classification**\n",
        "\n",
        "In this example we will be fine-tuning the model *BERT-base-uncased* to classify a sequence of tokens. For this purpose, we will use a PEFT method called **Adapter Fusion**, which creates a mixture of multiple pre-trained adapters. We will use **transformers** to download tokenizers, **datasets** for data download, **adapters** for creating adapters models and training, **evaluate** for loading evaluation metrics, and **wandb** (Weights & Biases) to log the results.\n",
        "\n",
        "You can also open this example in Google Colab:\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wicwik/peft_tutorial/blob/main/examples/adapter_fusion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **0. Install and import required modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jMXwq62JQ7I"
      },
      "outputs": [],
      "source": [
        "# 4.36.0 for compatibility with adapters\n",
        "# you will probably need to restart the sessions after installing these modules\n",
        "%pip install -q --user transformers[torch]==4.36.0\n",
        "%pip install -q --user datasets\n",
        "%pip install -q --user adapters\n",
        "%pip install -q --user evaluate\n",
        "# %pip install -q --user wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import evaluate\n",
        "import wandb\n",
        "import logging\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertConfig,\n",
        "    TrainingArguments,\n",
        "    default_data_collator\n",
        ")\n",
        "\n",
        "from adapters import BertAdapterModel, AdapterTrainer\n",
        "from adapters.composition import Fuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. Set variables**\n",
        "\n",
        "We will be fine-tuning the pre-trained version of model [bert-base-uncased](https://huggingface.co/google-bert/bert-large-uncased) which has **110M** parameters. We will set the max **input length to 128** tokens and train for **3 epochs** with **batch size of 32**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "model_name_or_path = \"bert-base-uncased\"\n",
        "tokenizer_name_or_path = \"bert-base-uncased\"\n",
        "\n",
        "max_length = 128\n",
        "lr = 1e-3\n",
        "num_epochs = 3\n",
        "batch_size = 32 # in case of \"unable to allocate\" errors, decrease the batch size to some lower number (e.g. 8 or 16)\n",
        "\n",
        "logging.disable(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2. Dataset and preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset that we will be using is called [Commitment Bank](https://huggingface.co/datasets/super_glue/viewer/cb) (CB) from the SuperGLUE benchmark. This dataset contains a set of premise-hypothesis pairs where the premise is a passage and the hypothesis is a clause. If the clause is contained within the passage and it is an entailment then the target is 0, for a contradiction it is 1, and 2 for a neutral clause.\n",
        "\n",
        "The dataset contains **250 training samples and 56 validation samples**. We will also split the validation part of the dataset in half to create a test part for evaluation after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'premise': 'It was a complex language. Not written down but handed down. One might say it was peeled down.',\n",
              " 'hypothesis': 'the language was peeled down',\n",
              " 'idx': 0,\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"super_glue\", \"cb\")\n",
        "\n",
        "# test set is not labeled so we need to do custom splits\n",
        "validtest = dataset[\"validation\"].train_test_split(test_size=0.5)\n",
        "\n",
        "dataset[\"validation\"] = validtest[\"train\"]\n",
        "dataset[\"test\"] = validtest[\"test\"]\n",
        "\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will tokenize the dataset. We only don't need to tokenize the labels this time, because we will train a classification head that returns numbers and not strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25e912878fed4fa7bbcc7d2fb9215bf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56964920fa59431ba42887174040a8f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "516c6205a46a4e9baf776a5543ad5107",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/250 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac83ac1eac3d4989a9cfccd1185496ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/28 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d46ae39575d245ee832670f671c9ae4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/28 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_name_or_path)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "dataset = dataset.map(preprocess_function, batched=True)\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=[\"premise\", \"hypothesis\", \"idx\"],\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "processed_datasets = processed_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset = processed_datasets[\"train\"].shuffle()\n",
        "eval_dataset = processed_datasets[\"validation\"]\n",
        "test_dataset = processed_datasets[\"test\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3. Create Adapters model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will create the adapter model and adapter fusion. At first, we load *BertAdapterModel* using the Adapters module, and after that, we will load **3 pre-trained adapters** to the model. These adapters are pre-trained on **MNLI** (cross-genre NLI), **QQP** (question paraphrase) and **QNLI** (QA NLI). As we don't need their prediction heads, we pass **with_head=False** to the loading method.\n",
        "\n",
        "After that we will add an adapter fusion layer, that combines all 3 adapter layers, activate it and set it trainable. The *train_adapter_fusion()* does two things: It freezes all weights of the model (including adapters!) except for the fusion layer and classification head. It also activates the given adapter setup to be used in the forward pass.\n",
        "\n",
        "Here is what the AdapterFusion layer looks like in the model:\n",
        "<p align=\"center\">\n",
        "<img src=\"../img/af.png\" alt=\"adapter_fusion_arch\" width=\"auto\" height=\"350\">\n",
        "<img src=\"../img/af_arch.png\" alt=\"adapter_fusion\" width=\"auto\" height=\"350\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Name                     Architecture         #Param      %Param  Active   Train\n",
            "--------------------------------------------------------------------------------\n",
            "multinli                 bottleneck          894,528       0.684       1       0\n",
            "qqp                      bottleneck          894,528       0.684       1       0\n",
            "qnli                     bottleneck          894,528       0.684       1       0\n",
            "--------------------------------------------------------------------------------\n",
            "Full model                               130,734,336     100.000               0\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertAdapterModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttentionWithAdapters(\n",
              "              (query): LoRALinear(\n",
              "                in_features=768, out_features=768, bias=True\n",
              "                (loras): ModuleDict()\n",
              "              )\n",
              "              (key): LoRALinear(\n",
              "                in_features=768, out_features=768, bias=True\n",
              "                (loras): ModuleDict()\n",
              "              )\n",
              "              (value): LoRALinear(\n",
              "                in_features=768, out_features=768, bias=True\n",
              "                (loras): ModuleDict()\n",
              "              )\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (prefix_tuning): PrefixTuningLayer(\n",
              "                (prefix_gates): ModuleDict()\n",
              "                (pool): PrefixTuningPool(\n",
              "                  (prefix_tunings): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (output): BertSelfOutputWithAdapters(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (adapters): ModuleDict()\n",
              "              (adapter_fusion_layer): ModuleDict()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): LoRALinear(\n",
              "              in_features=768, out_features=3072, bias=True\n",
              "              (loras): ModuleDict()\n",
              "            )\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutputWithAdapters(\n",
              "            (dense): LoRALinear(\n",
              "              in_features=3072, out_features=768, bias=True\n",
              "              (loras): ModuleDict()\n",
              "            )\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (adapters): ModuleDict(\n",
              "              (multinli): Adapter(\n",
              "                (non_linearity): Activation_Function_Class(\n",
              "                  (f): ReLU()\n",
              "                )\n",
              "                (adapter_down): Sequential(\n",
              "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
              "                  (1): Activation_Function_Class(\n",
              "                    (f): ReLU()\n",
              "                  )\n",
              "                )\n",
              "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
              "              )\n",
              "              (qqp): Adapter(\n",
              "                (non_linearity): Activation_Function_Class(\n",
              "                  (f): ReLU()\n",
              "                )\n",
              "                (adapter_down): Sequential(\n",
              "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
              "                  (1): Activation_Function_Class(\n",
              "                    (f): ReLU()\n",
              "                  )\n",
              "                )\n",
              "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
              "              )\n",
              "              (qnli): Adapter(\n",
              "                (non_linearity): Activation_Function_Class(\n",
              "                  (f): ReLU()\n",
              "                )\n",
              "                (adapter_down): Sequential(\n",
              "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
              "                  (1): Activation_Function_Class(\n",
              "                    (f): ReLU()\n",
              "                  )\n",
              "                )\n",
              "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (adapter_fusion_layer): ModuleDict(\n",
              "              (multinli,qqp,qnli): BertFusion(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "    (invertible_adapters): ModuleDict()\n",
              "    (shared_parameters): ModuleDict()\n",
              "    (prefix_tuning): PrefixTuningPool(\n",
              "      (prefix_tunings): ModuleDict()\n",
              "    )\n",
              "    (prompt_tuning): PromptTuningLayer(\n",
              "      (base_model_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (prompt_tunings): ModuleDict()\n",
              "    )\n",
              "  )\n",
              "  (heads): ModuleDict(\n",
              "    (default): BertStyleMaskedLMHead(\n",
              "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (1): Activation_Function_Class(\n",
              "        (f): GELUActivation()\n",
              "      )\n",
              "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (3): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "    (cb): ClassificationHead(\n",
              "      (0): Dropout(p=0.1, inplace=False)\n",
              "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (2): Activation_Function_Class(\n",
              "        (f): Tanh()\n",
              "      )\n",
              "      (3): Dropout(p=0.1, inplace=False)\n",
              "      (4): Linear(in_features=768, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label = {id: label for (id, label) in enumerate(processed_datasets[\"train\"].features[\"labels\"].names)}\n",
        "\n",
        "config = BertConfig.from_pretrained(model_name_or_path, id2label=id2label)\n",
        "\n",
        "# from transformers import AutoModelForSequenceClassification\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=config)\n",
        "model = BertAdapterModel.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "# comment everything from this line to the model and replace BertAdapterModel with BertForSequenceClassification to do FFT (using around 6GB of memory)\n",
        "model.load_adapter(\"nli/multinli@ukp\", load_as=\"multinli\", with_head=False)\n",
        "model.load_adapter(\"sts/qqp@ukp\", with_head=False)\n",
        "model.load_adapter(\"nli/qnli@ukp\", with_head=False)\n",
        "\n",
        "model.add_adapter_fusion(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "model.set_active_adapters(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "\n",
        "model.add_classification_head(\"cb\", num_labels=len(id2label))\n",
        "\n",
        "adapter_setup = Fuse(\"multinli\", \"qqp\", \"qnli\")\n",
        "model.train_adapter_fusion(adapter_setup)\n",
        "\n",
        "print(model.adapter_summary())\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We don't have a function for gen number of trainable parameters as in the Hugging Face PEFT module, but we can use [their implementation](https://github.com/huggingface/peft/blob/main/src/peft/peft_model.py#L492) also for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 22,467,645 || all params: 134,633,469 || trainable%: 16.688008685269782\n"
          ]
        }
      ],
      "source": [
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for n, param in model.named_parameters():\n",
        "    num_params = param.numel()\n",
        "    if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "        num_params = param.ds_numel\n",
        "\n",
        "    if param.__class__.__name__ == \"Params4bit\":\n",
        "        num_params = num_params * 2\n",
        "\n",
        "    all_param += num_params\n",
        "    if param.requires_grad:\n",
        "        # print(n)\n",
        "        trainable_params += num_params\n",
        "\n",
        "\n",
        "print(f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **4. Training and evaluation**\n",
        "\n",
        "We will be using Hugging Face [TrainingArguments](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments) and [AdapterTrainer](https://docs.adapterhub.ml/training.html#adaptertrainer) from Adapter Hub (this adapter is based on transformers adapter). The BERT model is not generating tokens, therefore we don't need to do any postprocessing and can use the metric form *evaluate.load()*. The trainer will take a *compute_metrics* method that will be used to compute metrics during the evaluation. \n",
        "\n",
        "For SuperGLUE CB dataset *evaluate* computes F1 and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"super_glue\", \"cb\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    preds = preds.argmax(axis=1)\n",
        "\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"out\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=num_epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will do the training and evaluation. We can have a look at the memory usage.\n",
        "\n",
        "Since the *AdapterTrainer* class is inherited from the Transformers *Trainer* class, we can see the trainer uploading results to the *wandb*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobert-belanec\u001b[0m (\u001b[33mrbelanec\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jovyan/peft_tutorial/examples/wandb/run-20240319_123327-ftdfvo75</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rbelanec/huggingface/runs/ftdfvo75' target=\"_blank\">soft-valley-93</a></strong> to <a href='https://wandb.ai/rbelanec/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rbelanec/huggingface' target=\"_blank\">https://wandb.ai/rbelanec/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rbelanec/huggingface/runs/ftdfvo75' target=\"_blank\">https://wandb.ai/rbelanec/huggingface/runs/ftdfvo75</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 00:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.907700</td>\n",
              "      <td>0.530087</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.593407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.517100</td>\n",
              "      <td>0.298829</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.564394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.156321</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.875556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'test_loss': 0.8526235818862915,\n",
              " 'test_accuracy': 0.7142857142857143,\n",
              " 'test_f1': 0.5128205128205128,\n",
              " 'test_runtime': 0.1152,\n",
              " 'test_samples_per_second': 242.986,\n",
              " 'test_steps_per_second': 34.712,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for FFT you also need to replace the AdapterTrainer with standard Trainer\n",
        "# from transformers import Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=eval_dataset,\n",
        "#     data_collator=default_data_collator,\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "\n",
        "# if wandb.run is not None:\n",
        "#     wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **5. Save and load**\n",
        "\n",
        "Now we can save the model with *save_adapter_fusion* and *save_all_adapters* methods to save the AdapterFusion layer and all trained adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "adapter_model_id = f\"{model_name_or_path}_adapterfusion_seqcls\"\n",
        "\n",
        "model.save_pretrained(adapter_model_id)\n",
        "model.save_adapter_fusion(adapter_model_id, \"multinli,qqp,qnli\")\n",
        "model.save_all_adapters(adapter_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can load the model and give it a custom example. It is important to **set active adapters** and to **specify the head** that we want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fuse[multinli, qqp, qnli]\n",
            "{'input_ids': tensor([[  101,  1037, 12063,  1012,  2005,  2870,  1010,  1037,  2307, 12063,\n",
            "          1012,  2021,  2053,  2028,  2064,  2360,  3387, 15451,  8566,  2378,\n",
            "          2038,  2025,  2363, 15250,  1012,   102,  3387, 15451,  8566,  2378,\n",
            "          2038,  2025,  2363, 15250,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "entailment 0\n"
          ]
        }
      ],
      "source": [
        "model = BertAdapterModel.from_pretrained(adapter_model_id)\n",
        "model.set_active_adapters(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "\n",
        "print(model.active_adapters)\n",
        "\n",
        "inputs = tokenizer(\"A pity. For myself, a great pity. But no one can say Bishop Malduin has not received latitude.\", \n",
        "                   \"Bishop Malduin has not received latitude\", \n",
        "                   return_tensors=\"pt\"\n",
        "                   )\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs, head=\"cb\")[0]\n",
        "    class_id = torch.argmax(logits).item()\n",
        "    pred_class = id2label[class_id]\n",
        "    print(pred_class, class_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [**References**](https://github.com/Wicwik/peft_tutorial/tree/main/references)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
