{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGg92CLzI5UN"
      },
      "source": [
        "# **Prompt Tuning for Sequence Classificaiton**\n",
        "\n",
        "In this example we will be fine-tuning the model *RoBERTa Large* to classify a sequence of tokens. For this purpose, we will use a PEFT method called **Prompt Tuning**, which prepends a trainable embedding matrix to the input embeddings. We will use **transformers** to download models and training, **datasets** for data downdload **peft** for Prompt Tuning model initialization, **evaluate** for loading evaluation metrics and **wandb** (Weights & Biases) to log the results.\n",
        "\n",
        "You can also open this example in google colab:\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wicwik/peft_tutorial/blob/main/examples/pt_classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **0. Install and import required modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.36.0 for compatibility with adapters\n",
        "%pip install -q --user transformers==4.36.0\n",
        "%pip install -q --user datasets\n",
        "%pip install -q --user peft\n",
        "%pip install -q --user evaluate\n",
        "# %pip install -q --user wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjqla9wQIw3H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# import wandb\n",
        "import evaluate\n",
        "\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    PromptTuningConfig,\n",
        "    TaskType,\n",
        "    PromptTuningInit,\n",
        ")\n",
        "from transformers import ( \n",
        "    AutoModelForSequenceClassification, \n",
        "    AutoTokenizer, \n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. Set variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be fine-tuning the pre-trained version of model [roberta-large](https://huggingface.co/FacebookAI/roberta-large) which has **355M** parameters. We will set the max **input length to 128** tokens and train for **3 epochs** with **batch size of 32**. \n",
        "\n",
        "For Prompt Tuning we will also set the *num_virtual_token* variable, which represents the lenght of the soft prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "model_name_or_path = \"roberta-large\"\n",
        "tokenizer_name_or_path = \"roberta-large\"\n",
        "\n",
        "max_length = 128\n",
        "lr = 1e-3\n",
        "num_epochs = 3\n",
        "batch_size = 32 # in case of \"unable to allocate\" errors, decrease batch size to some lower number (e.g. 8,16)\n",
        "num_virtual_tokens = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2. Create PEFT model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will create the PEFT model. The Hugging Face PEFT module will freeze the weights and add prompt encoder automatically. \n",
        "\n",
        "We are also passing the *prompt_tuning_init_text* parameter to initialize the soft prompt with a text, which also requires the models tokenizer to be speiciefied. In this case the init text is transformed to tokens and those tokens are than used to get embeddings from model's vocabulary. These embeddings are than used to initialize the prompt encoder weights. Since we will be working with paraphrases, we would like the model to determine, if the sentence is paraphrase.\n",
        "\n",
        "Compare the model architecutres with and without the added prompt encoder weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    num_virtual_tokens=num_virtual_tokens,\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    tokenizer_name_or_path=tokenizer_name_or_path,\n",
        "    prompt_tuning_init_text=\"Is the meaning of these sentences equivalent:\",\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
        "\n",
        "# comment next 2 lines if you want to do FFT\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see that we have been able to reduce the number of trainable parameters to mere **0.3% of original model parameters**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3. Dataset and preprocessing**\n",
        "\n",
        "The dataset that we will be using is called [Microsoft Research Paraphrase Corpus (MRPC)](https://huggingface.co/datasets/financial_phrasebank). It is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent. The dataset contains 5.8k samples from which are **3.67k samples used for training**. We don't need to do any splits, since the dataset is fully annotated and contains train, valid and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we have also a usable test split already, so we don't need to make it\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will tokenize the dataset. We only don't need to tokenize the labels because we will train model to return real numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    return model_inputs\n",
        "\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=[\"sentence1\", \"sentence2\", \"idx\"],\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "processed_datasets = processed_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset = processed_datasets[\"train\"].shuffle()\n",
        "eval_dataset = processed_datasets[\"validation\"]\n",
        "test_dataset = processed_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **4. Training and evaluation**\n",
        "\n",
        "For training we are using the Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) and provide it with [TrainingArguments](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments). The trainer will take a *compute_metrics* method that will be used to compute metrics during the evaluation. \n",
        "\n",
        "For GLUE MRPC dataset *evaluate* computes F1 and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    preds = preds.argmax(axis=1)\n",
        "\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"out\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=num_epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will do the traning and evaluation. Give a quick look on GPU memory usage, how much are we using? How would the memory usage change if we would do FFT?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "\n",
        "if wandb.run is not None:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **5. Save and load**\n",
        "\n",
        "Now we can save the model just with *save_pretrained* method (like we would for other Hugging Face transformers models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
        "model.save_pretrained(peft_model_id)\n",
        "\n",
        "ckpt = f\"{peft_model_id}/adapter_model.safetensors\"\n",
        "!du -h $ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now load the pre-trained model and give it a custom example.\n",
        "\n",
        "Notice that we have saved the last version of the model. In more real scenario, we would like to save the model with the best validation score and load it at the end of the training. We can do this with training args."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"this is an apple\", \"this is a fruit\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    print(outputs.logits.argmax(axis=1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
