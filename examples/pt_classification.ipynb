{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGg92CLzI5UN"
      },
      "source": [
        "# Prompt-Tuning for Sequence Classificaiton\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wicwik/peft_tutorial/blob/main/examples/pt_classification.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.35.2 for compatibility with adapters\n",
        "%pip install -q --user transformers==4.35.2\n",
        "%pip install -q --user datasets\n",
        "%pip install -q --user peft\n",
        "%pip install -q --user evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjqla9wQIw3H"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "\n",
        "    PromptTuningConfig,\n",
        "    TaskType,\n",
        "    PromptTuningInit,\n",
        ")\n",
        "\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import ( \n",
        "    AutoModelForSequenceClassification, \n",
        "    AutoTokenizer, \n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "model_name_or_path = \"roberta-large\"\n",
        "tokenizer_name_or_path = \"roberta-large\"\n",
        "\n",
        "task = \"mrpc\"\n",
        "max_length = 128\n",
        "lr = 1e-3\n",
        "num_epochs = 3\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "peft_config = PromptTuningConfig(task_type=TaskType.SEQ_CLS, num_virtual_tokens=10, prompt_tuning_init=PromptTuningInit.TEXT, tokenizer_name_or_path=tokenizer_name_or_path, prompt_tuning_init_text=\"Is the meaning of these sentences equivalent:\")\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
        "\n",
        "# comment this if you want to do FFT\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we have also a usable test split already, so we don't need to make it\n",
        "dataset = load_dataset(\"glue\", task)\n",
        "\n",
        "\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    return model_inputs\n",
        "\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=[\"sentence1\", \"sentence2\", \"idx\"],\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "processed_datasets = processed_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset = processed_datasets[\"train\"].shuffle()\n",
        "eval_dataset = processed_datasets[\"validation\"]\n",
        "test_dataset = processed_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"glue\", task)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"out\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=num_epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    preds = preds.argmax(axis=1)\n",
        "\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "\n",
        "if wandb.run is not None:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
        "model.save_pretrained(peft_model_id)\n",
        "\n",
        "ckpt = f\"{peft_model_id}/adapter_model.safetensors\"\n",
        "!du -h $ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"sentence1: this is an apple, sentence2: this is a pear\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=inputs[\"input_ids\"])\n",
        "    print(outputs.logits.argmax(axis=1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
