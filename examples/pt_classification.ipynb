{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGg92CLzI5UN"
      },
      "source": [
        "# **Prompt Tuning for Sequence Classificaiton**\n",
        "\n",
        "In this example we will be fine-tuning the model *RoBERTa Large* to classify a sequence of tokens. For this purpose, we will use a PEFT method called **Prompt Tuning**, which prepends a trainable embedding matrix to the input embeddings. We will use **transformers** to download models and training, **datasets** for data downdload **peft** for Prompt Tuning model initialization, **evaluate** for loading evaluation metrics and **wandb** (Weights & Biases) to log the results.\n",
        "\n",
        "You can also open this example in google colab:\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wicwik/peft_tutorial/blob/main/examples/pt_classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **0. Install and import required modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.35.2 for compatibility with adapters\n",
        "%pip install -q --user transformers==4.35.2\n",
        "%pip install -q --user datasets\n",
        "%pip install -q --user peft\n",
        "%pip install -q --user evaluate\n",
        "%pip install -q --user wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjqla9wQIw3H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import wandb\n",
        "import evaluate\n",
        "\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    PromptTuningConfig,\n",
        "    TaskType,\n",
        "    PromptTuningInit,\n",
        ")\n",
        "from transformers import ( \n",
        "    AutoModelForSequenceClassification, \n",
        "    AutoTokenizer, \n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. Set variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be fine-tuning the pre-trained version of model [roberta-large](https://huggingface.co/FacebookAI/roberta-large) which has **355M** parameters. We will set the max **input length to 128** tokens and train for **3 epochs** with **batch size of 32**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "model_name_or_path = \"roberta-large\"\n",
        "tokenizer_name_or_path = \"roberta-large\"\n",
        "\n",
        "max_length = 128\n",
        "lr = 1e-3\n",
        "num_epochs = 3\n",
        "batch_size = 32 # in case of \"unable to allocate\" errors, decrease batch size to some lower number (e.g. 8,16) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2. Create PEFT model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will create the PEFT model. The Hugging Face PEFT module will freeze the weights and add prompt encoder automatically.\n",
        "\n",
        "Compare the model architecutres with and without the added prompt encoder weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "peft_config = PromptTuningConfig(task_type=TaskType.SEQ_CLS, num_virtual_tokens=10, prompt_tuning_init=PromptTuningInit.TEXT, tokenizer_name_or_path=tokenizer_name_or_path, prompt_tuning_init_text=\"Is the meaning of these sentences equivalent:\")\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
        "\n",
        "# comment next 2 lines if you want to do FFT\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see that we have been able to reduce the number of trainable parameters to mere **0.2% of original model parameters**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3. Dataset and preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we have also a usable test split already, so we don't need to make it\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    return model_inputs\n",
        "\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=[\"sentence1\", \"sentence2\", \"idx\"],\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "processed_datasets = processed_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset = processed_datasets[\"train\"].shuffle()\n",
        "eval_dataset = processed_datasets[\"validation\"]\n",
        "test_dataset = processed_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **4. Training and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    preds = preds.argmax(axis=1)\n",
        "\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"out\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=num_epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "\n",
        "if wandb.run is not None:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **5. Save and load**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
        "model.save_pretrained(peft_model_id)\n",
        "\n",
        "ckpt = f\"{peft_model_id}/adapter_model.safetensors\"\n",
        "!du -h $ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"this is an apple\", \"this is a fruit\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    print(outputs.logits.argmax(axis=1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
