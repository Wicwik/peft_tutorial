# References

## Tools

[Adapters](https://github.com/adapter-hub/adapters)

[ðŸ¤— PEFT](https://github.com/huggingface/peft)

[Weights & Biases](https://wandb.ai/)

## Examples

[ðŸ¤— PEFT examples](https://github.com/huggingface/peft/tree/main/examples)

[adapter-hub examples](https://github.com/adapter-hub/adapters/tree/main/notebooks)

## Models

[mt0-large](https://huggingface.co/bigscience/mt0-large)

[roberta-large](https://huggingface.co/FacebookAI/roberta-large)

[bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)

## Papers

### LoRA

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

### Adapters and AdapterFusion
[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)

### Prompt tuning
[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)

### Review

[Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment](https://arxiv.org/pdf/2312.12148.pdf)